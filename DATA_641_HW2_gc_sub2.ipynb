{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA-641\n",
        "Max Calzada"
      ],
      "metadata": {
        "id": "E7TR8zL2na1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1** (35 points) \n",
        "\n",
        "Given a collection of COVID-19-related posts from various social media\n",
        "platforms such as Twitter, Facebook, Instagram, etc., our task will be to pre-process, and\n",
        "generate various types of features that can be used to train a model and classify a post as either\n",
        "real or fake. For HWK2 answer the following questions."
      ],
      "metadata": {
        "id": "aMXLJbX8nT8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) (1 points) \n",
        "\n",
        "Access and import the data from the TrainLabels.csv file."
      ],
      "metadata": {
        "id": "a9hD01sgng1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install gensim\n",
        "!pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rMxB1LN_lxwk",
        "outputId": "bbfb7d98-8eeb-4c48-f117-6ef82831fe9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.16.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (4.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code adapted from \n",
        "  # https://github.com/swarooprm/DQI/blob/master/1-2-3.ipynb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize # separates words into separate strings\n",
        "from nltk.tokenize import sent_tokenize # separates sentences into separate strings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer # identifies root word (\"running\" and \"ran\" are forms of \"run\")\n",
        "nltk.download('averaged_perceptron_tagger') # tags parts of speech (preposition, determiner, ...)\n",
        "\n",
        "# import BeautifulSoup\n",
        "# from BeautifulSoup import BeautifulStoneSoup\n",
        "import cgi\n",
        "import html\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "from collections import Counter # counts words, stores them in dictionary\n",
        "from itertools import chain # Groups all the iterables together and produces a single iterable as output\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from spacy.lang.en.examples import sentences\n",
        "\n",
        "# Code based off of:\n",
        "# https://stackoverflow.com/questions/56927602/unable-to-load-the-spacy-model-en-core-web-lg-on-google-colab # Max: Should I delete this line?\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request \n",
        "import matplotlib\n",
        "import re\n",
        "import nltk\n",
        "from gensim.models import Word2Vec, KeyedVectors #To load the model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LUphcO3yV0mz",
        "outputId": "6ef042d5-2a25-43bc-b1f7-171adbeab94f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('TrainLabels.csv')"
      ],
      "metadata": {
        "id": "HxYW5troYFzb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) For all posts within a category (real or fake), determine the total number of unique words, average words per post, and average characters per post. What do you observe about real news posts and fake news posts in terms of both characters per post and words per post?"
      ],
      "metadata": {
        "id": "NwO_JpsO3oX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collaborated with Dalia Habiby for (b)\n",
        "\n",
        "def tweet_stats(df):\n",
        "    \n",
        "    a = df.label.unique()\n",
        "    b= [a[0], a[1]]\n",
        "\n",
        "    for label in b:\n",
        "        newdf= df[df['label'] == label]\n",
        "        post_col = newdf['tweet']\n",
        "        post_lst = post_col.tolist()\n",
        "        string = ' '.join(post_lst)\n",
        "        words = string.split()\n",
        "        unique_words = len(set(words))\n",
        "        print(label, \"category unique words\", unique_words)\n",
        "    \n",
        "        w_per_post = []\n",
        "        chr_per_post = []\n",
        "\n",
        "        for post in post_lst:\n",
        "            \n",
        "            w_per_post.append(len(post.split()))\n",
        "            chr_per_post.append(len(post))\n",
        "    \n",
        "        print(label, \"average words\", np.mean(w_per_post))\n",
        "        print(label, \"average characters\", np.mean(chr_per_post))\n",
        "\n",
        "tweet_stats(tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2w0w8kDlAPPY",
        "outputId": "92ab83df-9bfc-450a-f603-c2a1a291134f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real category unique words 17502\n",
            "real average words 31.676488095238096\n",
            "real average characters 215.0613095238095\n",
            "fake category unique words 16396\n",
            "fake average words 21.865359477124183\n",
            "fake average characters 144.8875816993464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) (15 points) For all posts use NLTK or any other NLP Python module and perform the\n",
        "following:"
      ],
      "metadata": {
        "id": "_hluZQ3rVBEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# This is a standard pre-processing function\n",
        "def clean_text(str_list, lemmatize = True):\n",
        "    clean_list = [] # empty set\n",
        "    \n",
        "    for text in str_list:\n",
        "        # to drop pound sign from hash tags\n",
        "        text = re.sub(r'#', '', text) # gets rid of #\n",
        "\n",
        "        # Collaborated with Dalia Habiby for the following 5 lines\n",
        "        text = re.sub(r\"http\\S+\", \"\", text) # c (ii) # Remove urls form text\n",
        "        text = re.sub(r\"@ \\S+\", \"\", text) # c (ii) # remove words starting with @\n",
        "        text = re.sub(r\"@\\S+\", \"\", text) # c (ii) # remove words starting with @\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        text = html.unescape(text) # ? # c (v) # Convert XML entities into characters\n",
        "\n",
        "        text = text.lower() # (c) iv # lower case\n",
        "        words = word_tokenize(text) # tokenizes word\n",
        "        clean_words = []\n",
        "        \n",
        "        for word in words:            \n",
        "            # drop words with fewer than 2 characters; drop any punctuation \"words\"\n",
        "            if (len(word) > 1) and (re.match(r'^\\w+$', word)):\n",
        "\n",
        "                if lemmatize==True: # (c) iii # Lemmatization\n",
        "                    lemmatizer=WordNetLemmatizer()\n",
        "                    word1 = lemmatizer.lemmatize(word)\n",
        "                    #if word!=word1:\n",
        "                        #print(word1)\n",
        "                        #print(word)\n",
        "                    \n",
        "                clean_words.append(word1)\n",
        "        clean_text = ' '.join(clean_words)\n",
        "        clean_list.append(clean_text)\n",
        "    \n",
        "    return clean_list"
      ],
      "metadata": {
        "id": "3AGlFzZQ__bz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['clean_tweet'] = clean_text(tweets['tweet'])"
      ],
      "metadata": {
        "id": "wkYqU8FLAmmU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (c) i\n",
        "  # Remove all English stopwords\n",
        "\n",
        "tweets['sw-r_ct'] = tweets['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
      ],
      "metadata": {
        "id": "ChkTmHc1dcg0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Extract uni-grams and bi-grams derived from the bag of words representation of each pre-processed social media post.\n",
        "\n",
        "In addition, encode these features as TF-IDF values."
      ],
      "metadata": {
        "id": "k-rEQsMNktJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"bi-gram text representation\"\n",
        "bi_gram_text_rep = CountVectorizer(lowercase = True,  \n",
        "                        stop_words = 'english',\n",
        "                        ngram_range = (1,2))\n",
        "\n",
        "bow_rep = bi_gram_text_rep.fit_transform(tweets['sw-r_ct'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase = True,\n",
        "                        stop_words = 'english',\n",
        "                        ngram_range = (1,2))\n",
        "\n",
        "tf_idf_rep = tfidf.fit_transform(tweets['sw-r_ct'])\n",
        "\n",
        "bow_rep.shape, tf_idf_rep.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "J1cwFaDHlRHA",
        "outputId": "bfa9a047-7f30-4950-978d-6a7cf33423fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6420, 73068), (6420, 73068))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Extract features that reflect text understandibility using the different readability metrics. These features are based on properties such as words per sentence, characters per word, syllables per word, number of complex words, etc."
      ],
      "metadata": {
        "id": "LuTenYEGGLPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# flesch\n",
        "flesch = [textstat.flesch_reading_ease(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# Automated Readability Index\n",
        "ari = [textstat.automated_readability_index(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# Readability Consensus based upon other readability tests\n",
        "consensus = [textstat.text_standard(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# McAlpine EFLAW Readability Score\n",
        "eflaw = [textstat.mcalpine_eflaw(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# Reading Time\n",
        "readtime = [textstat.reading_time(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# Syllable Count\n",
        "syllable = [textstat.syllable_count(tweet) for tweet in tweets['tweet']]\n",
        "\n",
        "# Lexicon Count\n",
        "lexicon = [textstat.lexicon_count(tweet) for tweet in tweets['tweet']]"
      ],
      "metadata": {
        "id": "SE6q5TeWLmq6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Calculate the average readibility scores of the real post as well as the fake posts for each readibility grade. What do you observe?"
      ],
      "metadata": {
        "id": "jJeUuEeUHTeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/andreasvc/readability/tarball/master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "65Xw5cC8Gb67",
        "outputId": "a08383ed-66ed-43f6-aea7-668367124d3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/andreasvc/readability/tarball/master\n",
            "  Downloading https://github.com/andreasvc/readability/tarball/master\n",
            "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m35.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: readability\n",
            "  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=36403 sha256=33dd2e2525b5089693018911e224a6ed1e1980c703f362b9a507578a88f38660\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvu8h75c/wheels/2b/9e/87/3bbf4e61d79902dcb8a4315e715983319cfbd1bdb08f056d89\n",
            "Successfully built readability\n",
            "Installing collected packages: readability\n",
            "Successfully installed readability-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code based off of:\n",
        "  # https://pypi.org/project/readability/\n",
        "\n",
        "import readability\n",
        "\n",
        "def tweet_readability(df):\n",
        "    \n",
        "    a = df.label.unique()\n",
        "    b= [a[0], a[1]]\n",
        "\n",
        "    for label in b:\n",
        "        newdf= df[df['label'] == label]\n",
        "        post_col = newdf['tweet']\n",
        "        post_lst = post_col.tolist()\n",
        "\n",
        "        for post in post_lst:\n",
        "            \n",
        "            results = readability.getmeasures(post, lang='en')\n",
        "    \n",
        "        print(label, \"Flesch Reading Ease:\", results['readability grades']['FleschReadingEase'])\n",
        "\n",
        "tweet_readability(tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0eVoN5y9OWsN",
        "outputId": "d220891b-72b6-47fd-b307-47d0dfedc88d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real Flesch Reading Ease: 52.66918918918922\n",
            "fake Flesch Reading Ease: 112.08500000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fake posts tend to be more easily readible than real posts."
      ],
      "metadata": {
        "id": "l1AO0R7CRKLi"
      }
    }
  ]
}